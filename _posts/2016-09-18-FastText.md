---
layout: post
title: "FastText"
description: 
headline: 
modified: 2016-09-18
category: research
tags: [papers]
imagefeature: 
mathjax: true
chart: 
comments: true
featured: true
---


# Play with FastText

Tagsï¼šCoding Research

---

Recently, Facebook published its two word embedding papers, *Enriching Word Vectors with Subword Information* and *Bag of Tricks for Efficient Text Classification*

I have wrote something about the Bag of Tricks for Efficient text Classification, but nothing for another paper, so I decide to read both and make an assumption, firstly is the *Bag of Tricks for Efficient Text Classification*

### Bag of Tricks for Efficient Text Classification

This paper explores a simple and efficient baseline for text classification. The experiments show that text classifier **fastText** is often on pair with deep learning classifiers in terms of accuracy. We can train fastText on more than one billion words in less than ten minutes using a standard multicore CPU, and classify half a million sentences among 312K classes in less than a minute.

#### Architecture

A simple and efficient baseline for text classification is BOW + SVM, representation  + classification. Easypeasy
But, there is a problem, the class imbalance, if use BOW, more frequent words will affect less frequent words a lot. So, this is where the Doc2Vec starts. 

This is the Bag of Tricks's architecture. 

<center><img src="http://115.159.189.52/files/201609/BOT.png" width="400" alt="citation network"/>
</center>

This is very simple, it's very similar to the traditional word2vec and doc2vec. I don't know why it's such a big deal. 

It only replece the middle word to the label. Simple. 

They used it for sentiment analysis
Dataset they use: 
Xiang Zhang, Junbo Zhao, and Yann
LeCun. 2015. Character-level convolutional networks
for text classification. In NIPS.

Also, tag prediction, 
Dataset they use: 
YFCC100M dataset (Thomee et al., 2016)

This paper is related to another paper, which is also provided by Mikolov. And after reading the next paper, than I start to recognize why fasttext is so popular. 

### Enriching Word Vectors with Subword Information

At first glance, this paper has the very same idea of word2vec before. But, there is a slight difference. This comes from an idea that, each word of the vocabulary is a distinct vector. 

So, there is no parameter sharing between them. That's to say, for some words that are rarely appeared, the vector of word is not easy to get. But, they have neighbors, maybe their neighbors happens very often. Like, when I say, chrysanthemum in the garden is very beautiful. Maybe the word "chrysanthemum" rarely in the context, but we can know garden, and maybe garden is related to beautiful, so we can have some suspects. I think this is what the paper try to solve. 

And, get back to the paper, the general model is very much the same. 
The skip-gram model is to maimize the log-likelihood:

$$\sum_{t=1}^T\sum_{c\in C_t}\log p(w_c|w_t)$$

where the context $C_t$ is the set of indices of words surrounding $w_t$.

So, predicting a word's meaning can use: 

$$p(w_c|w_t)=\frac{e^{s(w_t, w_c)}}{\sum_{j=1}^W e^{s(w_t, j)}}$$

But there is one problem, which is, it only regard each word as individual, like we can only predict word $w_c$ with $w_t$, it's not possible to predict $w_c$ with all other $w_c$s. So, here is a solution:

We can use all other words' vector to predict the current vector. It means that we can use all the $w_c$s to predict one word $w_c$. How to do this?

Firstly, we need to add all other words' probabilities. From the $p(w_c|w_t)$, we can expand the negative log-likelihood of one word to:

$$\log(1 + e^{-s(w_t, w_c)}) + \sum_{n\in N_{t,c}} \log(1 + e^{s(w_t, n)})$$

where $N_{t,c}$ is a set of negative examples sampled from the vocabulary. we can make the loss function $l: x \to \log(1 + e^{-x})$, and it's the same. 

So, the paper mentioned that the negative sampling will help the score function by using word $w_t$ and context word $w_c$ by taking the product between word: $s(w_t, w_c) = u_{wt}^T v_{wc}$, where $u$ and $v$ are vectors in R. 

But, here they propose the **subword model**, because skip-gram will ignore the internal structure of words, so they give a different scroing function $s$. 

Given a word $w$, the $n$-grams between $w$ is defined as $G_w \subset \{1, 2, ..., G\}$. For each $n$-gram $g$, they give a vector representation $z_g$. So, the scoring function will be :

$$s(w, c) = \sum_{g \in G_w} \textbf{z}_g^T \textbf{v}_c$$

So, this will help making the $n$-grams better. The subword model can consider both single word's vector and the $n$-gram's vector, this can help understanding the word vector better. 

### Implementation
Here I want to talk a little bit about the implementation of FastText, as *Facebook* had alread published there source code, others had made the Python wrapper, links: 
[Jupyter for FastText][1]

I don't want to be verbose about the implementation, the link is well explained basically everything. I followed the guide and the results prove to be very similar to the guide. But the guide only uses the default parameter settings, maybe play with more parameters of FastText will make the performance better. 

### The end

The idea is actually interesting. In daily life, lots of time we will met the unbalancing between samples, the paper uses $n$-gram to "stablize" unbalancing words. Maybe the idea can expand to other fields. 


  [1]: http://nbviewer.jupyter.org/github/jayantj/gensim/blob/683720515165a332baed8a2a46b6711cefd2d739/docs/notebooks/Word2Vec_FastText_Comparison.ipynb