---
layout: post
title: "paper reading 20160919"
description: 
headline: 
modified: 2016-09-18
category: research
tags: [papers]
imagefeature: 
mathjax: true
chart: 
comments: true
featured: true
---

# Paper Reading 20160919

Today, I find a paper mentioned in FastText:

**Joint Learning of Character and Word Embeddings**

This seems a paper about Chinese Word Embeddings, let's see how that doing. 

Firstly, the paper address the key problem for the existing methods: in some languages, a meaningful words will be composed of several characters. So, they focus on Chinese language to consider both "inner" characters and "external" characters. Here "inner" means the word with more important factor and "external" means the whole meaning. 

So, the framework seems similar to word2vec, however, consider the Chinese words' difference, they made some corrections:

 - propose multiple-prototype character embeddings. So, for a word, there can be multiple vectors
 - and they only solve normal words, not the transliteral words. 

### Model
So, the model CWE is based on CBOW, 

**CBOW**

CBOW is very familiar to NLPers, CBOW can predict target words using neighbour words:

$$L(D) = \frac{1}{M}\sum_{i=K}^{M-K} \log Pr(x_i|x_{i-K}, ..., x_{i + K})$$

$K$ is the context window size of a target word. CBOW get the probability by softmax: 

$$Pr(x_i|x_{i-K}, ..., x_{i + K}) = \frac{exp(\textbf{x}_o^T\cdot \textbf{x}_i)}{\sum_{x_i'\in W} exp(\textbf{x}_o^T \cdot \textbf{x}_i')}$$
 
So, $\textbf{x}_o$ is the average of all context words:
$$\textbf{x}_o = \frac{1}{2K}\sum_{j=i-K, ..., i+K} \textbf{x}$$

**CWE**
CWE considers character to improve word embeddings. So, suppose the word sequence is $D = \{x_1, ..., x_M\}$, the context word $x_j$ is:

$$\textbf{x}_j = \textbf{w}_j\oplus \frac{1}{N_j}\sum_{k=1}^{N_j}\textbf{c}_k$$

$\textbf{w}_j$ is the word embedding of $x_j$, $\textbf{c}_k$ is the embedding of $k$-th character $\textbf{c}_k$ in $x_j$. The $\oplus$ operator can both using add and concatenate, but according to the paper, the author find the add is good enough, concatenate only adds the training time, but not add up the performance. So, by using add up, the $\textbf{x}_j$ is:

$$\textbf{x}_j=\frac{1}{2}[\textbf{w}_j + \frac{1}{N_j}\sum_{k=1}^{N_j}\textbf{c}_k]$$

To sum up, the paper combines the word's vector and neighbor's vector, and generate the vector. 

So, to solve previously mentioned problem, the **Multiple Prototype Character Embeddings**, the paper try to keep multiple vectors for one character, each corresponding to one of the meanings, there are three methods: 

 - Position-based character embeddings
 - Cluster-based character embeddings
 - Nonparametric cluster-based character embeddings. 

#### Position-based Character Embeddings

They take the character position into account. So, the *Begin, Middle, End* position will take account: 

<center>
<img src="http://115.159.189.52/files/201609/CWE_pos.png" width="600" alt="citation network"/>
</center>

So, as figure above, we can have context word $x_j$, its characters $\{c_1, ..., c_{N_j}\}$. So, different position within $x_j$ will lead to different embeddings. 

When building the embedding $\textbf{x}_j$, so the beginning character $c_1$ will lead $\textbf{c}_1^B$. So the embedding for $\textbf{x}_j$ can be rewritten as:

$$\textbf{x}_j = \frac{1}{2}(\textbf{w}_j + \frac{1}{N_j}(\textbf{c}_1^B + \sum_{k=2}^{N_j - 1}\textbf{c}_k^M + \textbf{c}_{N_j}^E))$$

So this is used the neighbor characters, instead of neighbor words, this idea is easy to anticipate. 

### Cluster-based Character Embeddings

So, it's possible to firstly cluster all occurrences of a character according to its context and form multiple prototypes of the character. 

The idea is as following:

<center>
<img src="http://115.159.189.52/files/201609/CWE_cluster.png" width="600" alt="citation network"/>
</center>

for a context word $x_j = \{c_1, ..., c_N\}$. $\textbf{c}_k^{r_k^{max}}$ is defined as :

$$r_k^{max} = \arg \max_{r_k} S(\textbf{c}_k^{r_k}, \textbf{v}_context)$$

where$$\textbf{v}_{context} = \sum_{t=j-K}^{j+K} \textbf{x}_t=\sum_{t=j-K}^{j+K}\frac{1}{2}(\textbf{w}_t + \frac{1}{N_t}\sum_{c_u \in x_t}\textbf{c}_u^{most})$$

It means that the context will consider neighbor words into consideration. $\textbf{c}_u^{most}$ is the character embedding most frequently chosen by $x_t$, After optimal cluster assignment, the embedding $\textbf{x}_j$ is:

$$\textbf{x}_j=\frac{1}{2}(\textbf{w}_j + \frac{1}{N_j}\sum_{k=1}^{N_j}\textbf{c}_k^{r_k^{max}})$$. 

So, this is just a method using the most similar vector as the neighbor and do the math. Seems like that. 

And there is a Nonparametric embedding, it did not cluster, but how to get a clustering? 

$$
r_k = 
\begin{cases}N_{ck} + 1, \ \text{if S } < \lambda \text{ for all } r_k\\
r_k^{max}, \text{     otherwise}
\end{cases}
$$

Just a simple nonparametric clustering algorithm. 

The basic idea is this, and the result seems pretty good. I think the clustering skip-gram is enlightening for some future research. 