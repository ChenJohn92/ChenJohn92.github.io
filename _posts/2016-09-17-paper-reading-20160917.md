---
layout: post
title: "Paper Reading 20160917"
description: 
headline: 
modified: 2016-09-17
category: Papers
tags: [Papers Research]
imagefeature: 
mathjax: true
chart: 
comments: true
featured: true
---


# Paper Reading 20160917

Tags：Papers Research

---

### Goal-Directed Inductive Matrix Completion

Today, I read is an KDD'16 paper. This is a paper proposed an improved Inductive Matrix Completion method, the difference is GIMC can learn the feature mapping in a supervised manner. 

As we know, for normal Matrix Completion, like SVD or PCA, they don't have extra information, so as they can only provide the know information matrix completion, they cannot do the feature prediction. 

Suppose the feature matrix is $A$, the $A$ is $n_x \times n_y$ dimensions. So standard matrix completion tries to recover the low-rank matrix by solving these problems:

$$\min_Z \frac{1}{2}||P_{\Omega}(A-Z)||_F^2 + \lambda_z|||Z|$$

$$\min_{W, H}\frac{1}{2}||P_{\Omega}(A-WH^T)||_F^2 + \lambda_{WH}(||W||_F^2 + ||H||_F^2)$$

$W$ and $H$ are the features for two parts, and $P_{\Omega}(\cdot)$ is the projection operator that only retains entries of the matrix.
One challenge for traditional matrix completion is that it cannot directly use side information and apply it to predict new and unseen data, or simplify computation. Thus, **Inductive Matrix Completion** is proposed to solve this issue. 

Specifically, we are given a set of features $X \in R^{n_x\times d_x}$ for the rows of A, and similarly for columns, $Y \in R^{n_y\times d_y}$. So, $X$ and $Y$ represents the row and column feature, respectively. 
So, the IMC can come to like this: 

$$\min_Z \frac{1}{2}||P_{\Omega}(A-XZY^T)||_F^2 + \lambda_z|||Z|$$

$$\min_{W, H}\frac{1}{2}||P_{\Omega}(A-XWH^TY^T)||_F^2 + \lambda_{WH}(||W||_
F^2 + ||H||_F^2)$$

so $W\in R^{d_x \times k}$ and $H \in R^{d_y\times k}$

One represent the $X$ features, one represent the $Y$ features

So, the IMC is pretty good, but it remains something wrong, the problem is that $X$ and $Y$ in IMC should be perfectly aligned with row and column spaces, so that:

$$Col(A)\subseteq Col(X), Row(A)\subseteq Col(Y)$$

where $Col(\cdot)$ and $Row(\cdot)$ correspond to the column and row space. 
so, to achieve a good performance, features $X$ and $Y$ should be strongly related to the feature matrix. But, what if the $X$ and $Y$ is noisy? So, they proposed the GIMC. 

**Nonlinear Feature Mapping for IMC**

So, as they believe the $X$ and $Y$ is noisy, they believe we can use a **Nonlinear** feature mapping method to solve this, so, $X \to \Phi_X(X), Y \to \Phi_Y(Y)$, nothing else changed. 

So the IMC can change to:

$$\min_Z\frac{1}{2}||P_{\Omega}(A-\Phi_X(X)Z\Phi_Y(Y)^T)||_F^2 + \lambda_z||Z||$$

The specific mapping to be used are many. We can choose the kernel, use the mapping function. E.g, 

$$x_i \to \phi_U(x_i)=\frac{1}{\sqrt{m}}[cos(u_1^T x_i), ..., cos(u_m^Tx_i),\\ sin(u_1^Tx_i), ..., sin(u_m^Tx_i)]$$

The $U$ is the projection directions sampled according to distribution from kernel functions. Kernels are varies, you can use RBF kernel, or others as well. 

But, using the feature mapping for solving IMC need a large number of projection to achieve good performance. So, there are drawbacks, to make improvment, a natural question to ask is if we can reduce the number of samples needed to perform the approximation. 

So here comes the Goal-directed IMC

**GIMC**

To overcome the issue above, GIMC tries to learn feature mapping automatically to benefit the model. They propose to learn the feature mappings and model parameters simultaneously in supervised manner, like this:

$$\min_{Z,U,V}\{\lambda_z||Z|| + \sum_{i,j\in \Omega}l(\phi_U(x_i)^TZ\phi v(y_j), A_{ij}\} := f(Z,U,V)$$

Here, $l$ is the loss function, $Z$ is the model and feature mapping of $X, Y$ are $\phi_U(\cdot)$ and $\phi_U(\cdot)$ which are parameterized by $U$ and $V$ respectively. Here the loss function is $l(a, b)=\frac{1}{2}(b-a)^2$

In the paper, to solve the $\phi_u, \phi_v$, author believes there are two algorithms, GIMC-LFF(Learned Fourier Features) and GIMC-LNYS(Learned Nystrom approximation). 

The algorithm is like:

![此处输入图片的描述][1]


  [1]: http://115.159.189.52/files/201609/GIMC.png