---
layout: page
title: "Paper Reading 20160912"
description: "Daily Readings"
modified: 2016-09-12
category: blogs
tags: [Papers Daily_Readings]
imagefeature: cover6.jpg
comments: true
share: true
mathjax: true
---


# Paper Reading 20160912

---

### Siamese Recurrent Architectures for Learning Sentence Similarity
This paper present a siamese adaptation of LSTM model for labeled data comprised of pairs of variable-length sequences. The model is applied to assess semantic similarity between sentences. 

Firstly, the paper uses the the so called "siamese network". Firstly, what is siamese? 
There is a meaning of siamese: Siamese, an informal term for conjoined or fused. So, should be conjoined. 
The Siamese Architecture is just a name, representing the conjoined model:
![此处输入图片的描述][1]
So, the Siamese Architecture's loss function is the difference between two models. Thus, for this task, they will join both models. The architecture is like this: 
![此处输入图片的描述][2]
As we can see, they focus on the two LSTMs, LSTMa and LSTMb, the two models firstly read in word-vectors. repreenting each word in context. 
Each sentence $x_1, x_2, ..., x_T$ is passed to the LSTM, which updates its hidden state at each sequence-index via the LSTM. The final representation is the sentence's encoding. For a given pair of sentences, they first use LSTM to represent the sentence, than using the Siamese archtecture to get the similarity.
Unlike typical language modeling RNN, this paper uses LSTM as encoder. And uses the Siamese structure to serve as difference counting. Simple. 

  [1]: http://image.slidesharecdn.com/blueprint-160602234526/95/deep-learning-convolutional-neural-networks-architectural-zoo-12-638.jpg
  [2]: https://ai2-s2-public.s3.amazonaws.com/figures/2016-03-25/6812fb9ef1c2dad497684a9020d8292041a639ff/2-Figure1-1.png