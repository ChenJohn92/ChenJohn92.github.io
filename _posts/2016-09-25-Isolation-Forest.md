---
layout: post
title: "Isolation Forest"
description: 
headline: 
modified: 2016-09-25
category: Research
tags: [research]
imagefeature: 
mathjax: true
chart: 
comments: true
featured: true
---


# Isolation Forest

Tagsï¼šResearch 

---

So, here today I am reading the paper about the isolation forest, this is a famous anomaly detection algorithm to find anomals in dataset in fast and accurate way. It's very popular in industry usage. I'm curious about how that functions, so I decide to look at it. 

So, iForest comes from iTree(isolation tree). This is pretty obvious. iForest can explicitly isolates anomalies instead of profiles normal points. So, this means the method only focus on the anomal point. 

In this paper, the term *isolation* means 'separating' an instance from the rest of the instances. Since anomalies are 'few and different' so they are easy to isolate. In a data-induced random tree, partitioning of instances are repeated recursively until all instances are isolated. 

The random partitioning produces noticeable shorter paths for anomalies since:

 - fewer anomalies lead to a smaller number of partitions. 
 - instances with distinguishable attribute are more easy be seperated. 

#### iTree

isolation Forest is just like random Forest, the iForest is consist of isolation trees. iTree is a completely generated by a random process.

Suppose there are $N$ datas in total. When building an iTree, we firstly sample $\phi$ items as the training sample for the tree. 

In the samples, one feature will be randomly selected, and the feature is will make a binary classification for the samples with a randomly generated number(between max and min). And the algorithm will iteratively repeat these steps, until meet the ending condition:

 - tree height reached max: $log_2(\phi)$
 - Data can not be seperated
 
When all the iTree is built, we can use the tree to predict data. When an item comes, the item will be put to all iTrees. In each iTree, the item will go from each braches of tree, until reaches the leaf node. And the length of the walk will be recorded: $h(x)$. 

For all the trees, the Anomal Score is the weighted combination:

$$s(x,n) = 2^{(-\frac{E({h(x)})}{c(n)})}$$

where $h(x)$ is the distance length, $E(h(x))$ is the average distance length for all trees in the forest. 

For the anomal score, it is like this:

 1. if the score is close to 1, it's more likely to be anormal
 2. if the score is less than 0.5, it can be regarded as normal data
 3. if all the score is around 0.5, then maybe there are not many anomal data in the set. 
 
So, this is all for iForest. 

Next time I will show something about implementation, maybe.